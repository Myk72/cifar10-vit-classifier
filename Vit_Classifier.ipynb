{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b70dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd1fb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "img_size = 32           # CIFAR-10 images are 32x32\n",
    "patch_size = 4          # (32/4=8 â†’ 8x8 = 64 patches)\n",
    "num_channels = 3        # RGB images\n",
    "num_patches = (img_size // patch_size) ** 2\n",
    "embed_dim = 192          # Must be divisible by num_heads\n",
    "num_heads = 8\n",
    "mlp_dim = 4 * embed_dim\n",
    "transformer_layers = 6  # Deeper network\n",
    "dropout_rate = 0.1\n",
    "learning_rate = 0.003\n",
    "weight_decay = 0.05\n",
    "epochs = 50\n",
    "warmup_epochs = 5       # For linear warmup\n",
    "\n",
    "# Normalization Parameters\n",
    "mean = [0.4914,0.4822, 0.4465]\n",
    "std = [0.2471, 0.2435, 0.2616]\n",
    "\n",
    "# Output Class\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f521c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform train for CIFAR-10\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# transform Test\n",
    "transform_test = transforms.Compose([\n",
    "    # We don't need other expression we used in training format\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Conv2d(\n",
    "            num_channels,\n",
    "            embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af29066",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # First Normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # self attention is being computed (query, key, and value).\n",
    "        self.attention = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                               dropout=dropout_rate,\n",
    "                                               batch_first=True)\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        # Second Normalization\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Multilayer Perceptron\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "            nn.Linear(mlp_dim, embed_dim),\n",
    "            nn.Dropout(p=dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_x = self.norm1(x)\n",
    "        attention_output, _ = self.attention(norm_x, norm_x, norm_x)\n",
    "\n",
    "        # Residual connection\n",
    "        x = x + self.attention_dropout(attention_output)\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding()\n",
    "\n",
    "        # CLS embedding\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "        # Positional embedding\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "        self.transformer = nn.Sequential(*[TransformerEncoder() for _ in range(transformer_layers)])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, 10)  # 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, patch_inputs):\n",
    "        B = patch_inputs.shape[0]\n",
    "        patch_inputs = self.patch_embedding(patch_inputs)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1) # [B, 1, embed_dim]\n",
    "        patch_inputs = torch.cat((cls_tokens, patch_inputs), dim=1) # concatenation\n",
    "        patch_inputs = patch_inputs + self.positional_embedding\n",
    "\n",
    "        patch_inputs = self.transformer(patch_inputs) \n",
    "        patch_inputs = patch_inputs[:, 0] # Only the CLS token is selected\n",
    "        return self.head(patch_inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
